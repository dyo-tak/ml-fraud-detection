


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
pd.set_option('display.max_columns', None)
plt.style.use('seaborn-v0_8-darkgrid')

#Sampling methods
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE

#Feature engineering
import datetime as dt
import category_encoders as ce
from sklearn.preprocessing import MinMaxScaler

#Models
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

#Model evaluation
from sklearn.metrics import f1_score
from sklearn.metrics import roc_curve
from sklearn.metrics import auc
from sklearn.metrics import classification_report
from sklearn import metrics
import scipy.stats as stats
from scipy.stats import skew

#Ignore warning
import warnings 
warnings.filterwarnings('ignore')





train_df = pd.read_csv('../data/fraudTrain.csv')


#1296675,22
train_df.shape


train_df.head(3)


train_df.isnull().sum()


train_df = train_df.drop(columns=['Unnamed: 0'])


train_df.shape





train_df.describe().T


train_df.dtypes.value_counts().plot(kind='bar', color="orange");


dtype_counts = train_df.dtypes.value_counts()
ax = dtype_counts.plot(kind='bar', color="orange")

# Add text annotations for each bar
for i, count in enumerate(dtype_counts):
    ax.text(i, count + 0.1, str(count), ha='center', va='bottom')

# Set labels and title
plt.xlabel('Data Types')
plt.ylabel('Count')
plt.title('Count of Data Types in DataFrame')

# Show the plot
plt.show()





donut = train_df["is_fraud"].value_counts().reset_index()

labels = ["No", "Yes"]
explode = (0, 0)

fig, ax = plt.subplots(dpi=120, figsize=(8, 4))
plt.pie(donut["count"],
        labels=donut["count"],
        autopct="%1.1f%%",
        pctdistance=0.8,
        explode=explode)

centre_circle = plt.Circle((0.0, 0.0), 0.5, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title("Fraud proportion in Transactions")
plt.legend(labels, loc="center", frameon=False)
plt.show();


sns.kdeplot(train_df["amt"], fill=True);


p99 = train_df["amt"].quantile(0.99)
sns.kdeplot(x="amt", data=train_df[train_df["amt"] <= p99], fill=True)





p99 = train_df["amt"].quantile(0.99)
sns.histplot(x="amt", hue="is_fraud", bins=30,
             stat="probability", data=train_df[train_df["amt"] <= p99],
             common_norm=False);


categories = train_df['category'].unique()

num_plots = len(categories)
num_rows = math.isqrt(num_plots)
num_cols = math.ceil(num_plots / num_rows)

fig, axes = plt.subplots(num_rows, num_cols, figsize=(
    5*num_cols, 5*num_rows), sharex=True)

for i, category in enumerate(categories):

    row = i // num_cols
    col = i % num_cols

    data_category = train_df[train_df['category'] == category]

    if num_rows == 1 and num_cols == 1:
        ax = axes
    elif num_rows == 1 or num_cols == 1:
        ax = axes[i]
    else:
        ax = axes[row, col]

    sns.histplot(x='amt', data=data_category[data_category['amt'] <= p99],
                 hue='is_fraud', stat='probability',
                 common_norm=False, bins=30, ax=ax)

    ax.set_ylabel('Percentage in Each Type')
    ax.set_xlabel('Transaction Amount in USD')
    ax.set_title(f'{category}')
    ax.legend(title='Type', labels=['Fraud', 'Not Fraud'])

plt.tight_layout()

plt.show();


# non_fraud
non_fraud = train_df[train_df['is_fraud'] == 0]['category'].value_counts(
    normalize=True).to_frame().reset_index()
non_fraud.columns = ['category', 'not_fraud_percentual_vs_total']

# fraud
fraud = train_df[train_df['is_fraud'] == 1]['category'].value_counts(
    normalize=True).to_frame().reset_index()
fraud.columns = ['category', 'fraud_percentage_vs_total']

# merging two dataframes and calculating "fraud level"
non_fraud_vs_fraud = non_fraud.merge(fraud, on='category')
non_fraud_vs_fraud['fraud_level'] = non_fraud_vs_fraud['fraud_percentage_vs_total'] - \
    non_fraud_vs_fraud['not_fraud_percentual_vs_total']

non_fraud_vs_fraud


custom_palette = sns.color_palette("flare")
ax = sns.barplot(y='category', x='fraud_level',
                 data=non_fraud_vs_fraud.sort_values('fraud_level', ascending=False), palette=custom_palette)
ax.set_xlabel('Percentage Difference')
ax.set_ylabel('Transaction Category')
plt.title('Fraud Level');


train_df['age'] = dt.date.today().year-pd.to_datetime(train_df['dob']).dt.year
ax = sns.kdeplot(x='age', data=train_df, hue='is_fraud', common_norm=False)
ax.set_xlabel('Credit Card Holder Age')
ax.set_ylabel('Density')
plt.xticks(np.arange(0, 110, 10))
plt.title('Age Distribution')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud']);


train_df['hour'] = pd.to_datetime(train_df['trans_date_trans_time']).dt.hour
f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), sharey=True)
ax1 = sns.histplot(x='hour', data=train_df[train_df["is_fraud"] == 0],
                   stat="density", bins=24, ax=ax1)
ax2 = sns.histplot(x='hour', data=train_df[train_df["is_fraud"] == 1],
                   stat="density", bins=24, ax=ax2, color="blue")
ax1.set_title("Normal")
ax2.set_title("Fraud")
ax1.set_xticks(np.arange(1, 24))
ax2.set_xticks(np.arange(1, 24));


train_df['month'] = pd.to_datetime(train_df['trans_date_trans_time']).dt.month
f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), sharey=True)
ax1 = sns.histplot(x='month', data=train_df[train_df["is_fraud"] == 0],
                   stat="density", bins=12, ax=ax1)
ax2 = sns.histplot(x='month', data=train_df[train_df["is_fraud"] == 1],
                   stat="density", bins=12, ax=ax2, color="blue")
ax1.set_title("Normal")
ax2.set_title("Fraud")
ax1.set_xticks(np.arange(1, 13))
ax2.set_xticks(np.arange(1, 13));








train_df.drop(columns=["merchant", "first", "last", "street",
           "unix_time", "trans_num"], inplace=True)



train_df.head(3)





train_df["amt_log"] = np.log1p(train_df["amt"])
sns.kdeplot(train_df["amt_log"], fill=True);


def check_normality(feature):
    plt.figure(figsize=(8, 8))
    ax1 = plt.subplot(1, 1, 1)
    stats.probplot(train_df[feature], dist=stats.norm, plot=ax1)
    ax1.set_title(f'{feature} Q-Q plot', fontsize=20)
    sns.despine()

    mean = train_df[feature].mean()
    std = train_df[feature].std()
    skew = train_df[feature].skew()
    print(f'{feature} : mean: {mean:.2f}, std: {std:.2f}, skew: {skew:.2f}')


check_normality("amt");


check_normality("amt_log");





def apply_woe(train, columns, target_col):
    woe = ce.WOEEncoder()

    for col in columns:
        X = train[col]
        y = train[target_col]

        new_col_name = f"{col}_WOE"
        train[new_col_name] = woe.fit_transform(X, y)

    return train


columns_to_encode = ["category", "state", "city", "job"]
target_column = "is_fraud"

train_df = apply_woe(train_df, columns_to_encode, target_column)


gender_mapping = {"F": 0, "M": 1}

train_df["gender_binary"] = train_df["gender"].map(gender_mapping)


freq_enc = (train_df.groupby("cc_num").size())
freq_enc.sort_values(ascending=True)
train_df["cc_num_frequency"] = train_df["cc_num"].apply(lambda x: freq_enc[x])


sns.histplot(train_df["cc_num_frequency"], bins=6);


intervals = [600, 1200, 1800, 2400, 3000, 3600]


def classify_frequency(freq):
    for i, c in enumerate(intervals):
        if freq <= c:
            return i


train_df["cc_num_frequency_classification"] = train_df["cc_num_frequency"].apply(
    classify_frequency)


f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), sharey=True)
ax1 = sns.histplot(x='cc_num_frequency_classification', data=train_df[train_df["is_fraud"] == 0],
                   stat="density", bins=6, ax=ax1)
ax2 = sns.histplot(x='cc_num_frequency_classification', data=train_df[train_df["is_fraud"] == 1],
                   stat="density", bins=6, ax=ax2, color="blue")
ax1.set_title("Normal")
ax2.set_title("Fraud");





X = train_df.drop(columns=["is_fraud"])
y = train_df["is_fraud"]


rus = RandomUnderSampler(sampling_strategy=0.1, random_state=23)


X_ru, y_ru = rus.fit_resample(X, y)


donut = y_ru.value_counts().reset_index()

labels = ["No", "Yes"]
explode = (0, 0)

fig, ax = plt.subplots(dpi=120, figsize=(8, 4))
plt.pie(donut["count"],
        labels=donut["count"],
        autopct="%1.1f%%",
        pctdistance=0.8,
        explode=explode)

centre_circle = plt.Circle((0.0, 0.0), 0.5, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title("Fraud Proportion with Undersampling")
plt.legend(labels, loc="center", frameon=False)
plt.show();





smote = SMOTE(sampling_strategy=0.1, random_state=23)


X_temp = X.drop(columns=["trans_date_trans_time",
                                       "city", "state", "category", "gender", "dob", "job", "cc_num", "amt"])


X_smote, y_smote = smote.fit_resample(X_temp, y)


donut = y_smote.value_counts().reset_index()

labels = ["No", "Yes"]
explode = (0, 0)

fig, ax = plt.subplots(dpi=120, figsize=(8, 4))
plt.pie(donut["count"],
        labels=donut["count"],
        autopct="%1.1f%%",
        pctdistance=0.8,
        explode=explode)

centre_circle = plt.Circle((0.0, 0.0), 0.5, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title("Fraud Proportion with Oversampling")
plt.legend(labels, loc="center", frameon=False)
plt.show();





X_ru["random_feat_1"] = np.random.randint(
    0, 100, size=len(X_ru))
X_ru["random_feat_2"] = np.random.randint(
    0, 500, size=len(X_ru))


X_train = X_ru.drop(columns=["trans_date_trans_time",
                                       "city", "state", "category", "gender", "dob", "job", "cc_num", "amt"])

y_train = y_ru


X_train.head(3)


rf = RandomForestClassifier()
rf.fit(X_train, y_train)
importances = rf.feature_importances_


forest_importances = pd.Series(importances, index=X_train.columns)

fig, ax = plt.subplots(figsize=(12, 5))
forest_importances.sort_values(ascending=False).plot.bar(ax=ax)
ax.set_ylabel("Feature Importance")
fig.tight_layout();


selected_columns = ['merch_lat', 'age', 'hour', 'amt_log', 'category_WOE', 'city_WOE', 'job_WOE', 'cc_num_frequency']
X_train_ru = X_train[selected_columns].copy()
X_train_ru.head()


X_train_smote = X_smote[selected_columns].copy()
X_train_smote.head()





test = pd.read_csv("../data/fraudTest.csv")


#555719
test.shape


test['age'] = dt.date.today().year-pd.to_datetime(test['dob']).dt.year
test['hour'] = pd.to_datetime(test['trans_date_trans_time']).dt.hour
test['month'] = pd.to_datetime(test['trans_date_trans_time']).dt.month

test.drop(columns=["merchant", "first", "last", "street",
                   "unix_time", "trans_num"], inplace=True)

test["amt_log"] = np.log1p(test["amt"])

test = apply_woe(test, columns_to_encode, target_column)

test["gender_binary"] = test["gender"].map(gender_mapping)

freq_enc_test = (test.groupby("cc_num").size())
freq_enc_test.sort_values(ascending=True)
test["cc_num_frequency"] = test["cc_num"].apply(lambda x: freq_enc_test[x])
test["cc_num_frequency_classification"] = test["cc_num_frequency"].apply(
    classify_frequency)

X_test = test.drop(columns=["trans_date_trans_time",
                            "city", "state", "category", "gender", "dob", "job", "cc_num", "amt", "is_fraud"])

y_test = test["is_fraud"]

X_test.drop(columns=["gender_binary", "state_WOE", "zip", "long", "lat",
                     "city_pop", "month", "cc_num_frequency_classification", "merch_long"], inplace=True)


def evaluate_model(target, predicted, y_score, model_name, normalize_matrix=None):
    accuracy = metrics.accuracy_score(target, predicted)
    precision = metrics.precision_score(target, predicted)
    recall = metrics.recall_score(target, predicted)
    f1 = f1_score(target, predicted)
    auc = metrics.roc_auc_score(target, y_score)

    confusion_matrix = metrics.confusion_matrix(target, predicted, normalize=normalize_matrix)
    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=[False, True])
    cm_display.plot()
    plt.grid(False)
    plt.title('Confusion Matrix for ' + model_name)
    plt.show()

    fpr, tpr, threshold = roc_curve(target, y_score)
    plt.plot(fpr, tpr, label="Model", c="blue")
    plt.plot([0, 1], [0, 1], linestyle="--", c="yellow")
    plt.title("ROC Curve for " + model_name)
    plt.legend()
    plt.show()

    metrics_df = pd.DataFrame({
        'Model': [model_name],
        'Accuracy': [accuracy],
        'Precision': [precision],
        'Recall': [recall],
        'F1 Score': [f1],
        'AUC': [auc]
    })

    print(metrics_df)
    return metrics_df


rf_ru = RandomForestClassifier(random_state=23)
rf_smote = RandomForestClassifier(random_state=23)
knn_ru = KNeighborsClassifier()
knn_smote = KNeighborsClassifier()
gboost_ru = GradientBoostingClassifier(random_state=23)
xgboost_ru = XGBClassifier()



X_test.drop(columns=['Unnamed: 0'], inplace=True)


X_test.head()


rf_ru.fit(X_train_ru, y_train)



y_pred_train = rf_ru.predict(X_train_ru)
y_score_train = rf_ru.predict_proba(X_train_ru)[:,1]
training_metrics = evaluate_model(y_train, y_pred_train, y_score_train, model_name="Training", normalize_matrix=None)


y_pred_test = rf_ru.predict(X_test)
y_score_test = rf_ru.predict_proba(X_test)[:,1]
rf_ru_training_metrics = evaluate_model(y_test, y_pred_test, y_score_test, model_name="Random Forest Undersampling", normalize_matrix=None)


knn_ru.fit(X_train_ru, y_train)
y_pred_test = knn_ru.predict(X_test)
y_score_test = knn_ru.predict_proba(X_test)[:,1]
knn_ru_training_metrics = evaluate_model(y_test, y_pred_test, y_score_test, model_name="KNN Undersampling", normalize_matrix=None)


gboost_ru.fit(X_train_ru, y_train)
y_pred_test = gboost_ru.predict(X_test)
y_score_test = gboost_ru.predict_proba(X_test)[:,1]
gboost_ru_training_metrics = evaluate_model(y_test, y_pred_test, y_score_test, model_name="Gradient Boost Undersampling", normalize_matrix=None)


xgboost_ru.fit(X_train_ru, y_train)
y_pred_test = xgboost_ru.predict(X_test)
y_score_test = xgboost_ru.predict_proba(X_test)[:,1]
xgboost_smote_training_metrics = evaluate_model(y_test, y_pred_test, y_score_test, model_name="XGboost Undersampling", normalize_matrix=None)


combined_df = pd.concat([
    rf_ru_training_metrics,
    knn_ru_training_metrics,
    gboost_ru_training_metrics,
    xgboost_smote_training_metrics
], ignore_index=True)


combined_df_sorted = combined_df.sort_values(by='Recall', ascending=False)
combined_df_sorted.set_index('Model', inplace=True)
def highlight_max(s):
    is_max = s == s.max()
    return ['background-color: orange' if v else '' for v in is_max]
    
styled_df = combined_df_sorted.style.apply(highlight_max, axis=0)
styled_df


from joblib import dump


dump(rf_ru, "../models/rf_ru.pkl")


dump(knn_ru, "../models/knn_ru.pkl")


dump(gboost_ru, "../models/gboost_ru.pkl")


dump(xgboost_ru, "../models/xgboost_ru.pkl")



